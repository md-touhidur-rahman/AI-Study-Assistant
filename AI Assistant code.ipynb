{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSbpvKR3rWlG"
   },
   "source": [
    "# 🧠 ClassMind — AI Study Assistant (DE/EN)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/<YOUR_USERNAME>/<YOUR_REPO>/blob/main/AI_Study_Assistant_fixed.ipynb)\n",
    "\n",
    "**How to use:** Runtime → **Run all**, then upload a PDF/PPTX when prompted. Outputs are saved to `/outputs`:\n",
    "`deck_summary.txt`, `slide_summaries.json`, `mindmap.mmd`, `flashcards.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uxXVCcT3raN7",
    "outputId": "20e99862-ea88-4c94-fe13-9a66f1c22f71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package tesseract-ocr-deu.\n",
      "(Reading database ... 126675 files and directories currently installed.)\n",
      "Preparing to unpack .../tesseract-ocr-deu_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
      "Unpacking tesseract-ocr-deu (1:4.00~git30-7274cfa-1.1) ...\n",
      "Setting up tesseract-ocr-deu (1:4.00~git30-7274cfa-1.1) ...\n"
     ]
    }
   ],
   "source": [
    "#  Setup\n",
    "!pip -q install pymupdf python-pptx pillow transformers sentence-transformers scikit-learn tqdm pytesseract pandas\n",
    "!sudo apt-get -qq update && sudo apt-get -qq install -y tesseract-ocr tesseract-ocr-deu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728,
     "referenced_widgets": [
      "9200425b0a8f4fb68ba570b144643684",
      "2d67130115c7473da5e127915f96921f",
      "ddd9f548985447b2ae8e8571604f7327",
      "d39509d21e9e48ee985cc1f355c0e851",
      "351907507dfa49349b9379721e66a87c",
      "f594daad0d6b4f9aa280f18c5c2ef1d2",
      "e74f227c592d4c0bb59dedd83d4edc1d",
      "05260224f02e480692c59ccebcea0bd7",
      "732ac2b8dae14e96816907a478ba91d3",
      "c23fed42c0604a1e8e3c4de3dccfd02b",
      "82b775a4109c48a6ba5509c4b780092c",
      "61059f739d1e4653b6ce2e6a951b19b2",
      "2c809565312e458ab4cc14713afbec23",
      "c91c031046924a928962ba167fbb9c60",
      "4ee513eda84c4dd1bef503d2cf532081",
      "215bd533c277428d9c42841df550a50d",
      "78e491ed6dbb4153b1ade75d7cc179d2",
      "f7ccbb31ee404e77888c43439edff1bd",
      "664c2110788b445caa030f0c7924ae72",
      "5569ed85b2a74191ae2ed1040d29bbcc",
      "4745283f00ba40f38a9f8e3e5feb2918",
      "8a43b9eb3b0a428baf17d87c10b40e84",
      "b553c89e9e4848da9b06961dbc013412",
      "162a7ce483cb4dd49b5ead6fd94ec72f",
      "648a066494b648939251f5b14578bd89",
      "fac86b0dcb004847806219f5816ffd0c",
      "58bbecd3a20f435599767e04b6ff582c",
      "d0326bafcd5743a88a2305643fcd48d4",
      "b69aaa89dff54a7c8fdff67c778e9242",
      "07511999a70642e6bb9b66dd94dcdfa0",
      "c4eaa90ee81d402c89f6639428d494bb",
      "d211b67ffed64bf389757e1637ab9030",
      "175ca4ba1c9b4d6591042c763d5533c6",
      "c6c611c1105048bbb60c35d38e7ae5b2",
      "ad9c0b3e027f41199422a484f734ebc8",
      "15ef0906e45449a49c75d69256a2f2db",
      "7e6e98ab76554307b264457726ce9eb1",
      "b5ffa93d8aed4c9e9213ad5fdf34e8fa",
      "2e6cf26ab9af446e962b8cb6a543c249",
      "746375a6588543cab1cc931735605f00",
      "948e5af06aca40baabcddef04e0d368d",
      "b9b18573cc79434b991cbf630076a87e",
      "ea369bf45a76443f9602182b1b17aaa3",
      "3b07b755501d4a5b9f2b537918667c38",
      "2b98ee775d73480389064eb81f2001ab",
      "bcbbf5c5ea2148dca4900d5242b3096d",
      "feee97afb96c495381201993dfbe37d0",
      "0942a1a49d9643c9a6e0630e2a0bafdc",
      "e57af34874484db0b5ca6f32045bf556",
      "11534affe65e4e7e9be3680e7040ca49",
      "590b4fbc20784cb1bc8ec73f4b7075bc",
      "b234a2ddd4ee4bf1ac7c402b00bfabf8",
      "efad4bd69542484ba24f70fe663f85e2",
      "a423c72f512249a7a025d5a80784b5f7",
      "ec4c74d5b79948a392572c9c37da3966",
      "5729501d73134e2188e1d28672dfa67f",
      "81542b2dfcf14c229410f79af0e13f33",
      "c5da62801fd846508e46b93d845052b4",
      "f96450dbce0a44ccb9bc36a0f8355cdd",
      "7ad33ea1a8964bf78f3b86455f05acfe",
      "db49ded708d8456594f0621f6ee69d4e",
      "2dcd3f4a623048579d4a6b43d455f43b",
      "0d1d78bdd55b4561aaabddc6c48adc3e",
      "6c06db65f6304c70a5963dd7cb9a1f1d",
      "96b8127ed54040669b6e232f2b0cfb80",
      "55a2e8447b0f4e2488d793db812981c9",
      "83e849126b474fae828febb009cd9136",
      "ba3a1e06f121425b8355c0fdb6916ffb",
      "f0217843d99d44c7bf1b3a06e8b4c267",
      "3828e64310384dbe8392fd08dee43a50",
      "ef8274fad5c24d23bc913672d0495bf4",
      "601830ae01464f1c9404a55aec238df7",
      "a4aa241beb7442d8bd08b5016c49cdb6",
      "a22f6c69ca4440beb636c659de18bd38",
      "1621ad2a4de44f6db6638aa47e89ddbe",
      "f7cba9d6554a4ec7a19bddad63fcd3f0",
      "9de630486f7746f8b5a298b842c88ca3",
      "cdd9bf3e37674151928589891b452d17",
      "cbe51d4fd0f34883be54fa1b48f84d45",
      "33fa7d686984403f80c5ada99bad63d2",
      "675017c1881b43618d74603ea87c2c4b",
      "e684a11db260418e9bdd6ebc149459c7",
      "f0e3b38fa02b44dda8971602225b57cd",
      "e0e61a85a59849d7a9db9c05f0bd57e9",
      "a8f430cfb23e4f32b02d80aadce8aefc",
      "a71d4315656d438785c9796a58f95e15",
      "059f009978c4472e8530af42e44665e1",
      "15f88baa591b463ebc2b0feb641f26f3",
      "d1a68555b8fb40d0b8dfcf2085d8daa8",
      "ad331ca536b64aaf91c417b21b7e083b",
      "b3b19a7bea104f3fadef66f8efd2664b",
      "840a928e527a4fce848cf30e960ee3e5",
      "f3d02162e4f4421ab2a37f15bb52c049",
      "75641582dc774b869ee786f0f146e1a3",
      "1d2bb6e92022455a95f93a6e0a3f5b65",
      "d9566e0e2cf04b25a3f39e9b209b1743",
      "1e19b4162d73421b948565949bc75732",
      "0d24da354cc34005974609e938493c6d",
      "a2cd1a7a2a5e43b5b31c5f2b71c9c1d2",
      "8ccc09fe3f5e43209282a33e8cf12680",
      "40a17b131aa142c1a55538b2d0458ceb",
      "0c3d0b9bebee45f2b9011deb8fa0f743",
      "6bddeb15add34c61af915b0fe7e93800",
      "d09f506386ab4d5881bc1b91ae4fed7a",
      "61fce62464b5491ea361e54b5d097714",
      "edf5368dd2bc4998ae01a7a272990e4c",
      "cb9f7fb8c1d34d29b71a120c82da7bb7",
      "bd7f8e025a0f4aa2b754c3e724fca083",
      "353e0f1b66394636bcff5537f9ff433c",
      "643d406e329c42c9b372e5ef11c5bbf8",
      "5489e6da731d4a418c816052effab430",
      "79df1cd873204704b5f4aa80a15b100c",
      "465b6e18969b4f2bbcdff2ec45102dae",
      "3f693515b5354a7397d98ac44afd7583",
      "4fbfd4bf6919411985fa904fae3b59dc",
      "f790d9e812504700b490be0efbcd6238",
      "61207fe7cbbb4f83b99da5e607f14732",
      "87b0a6a7f4a845ee93c79497167b0d34",
      "923ecfd423a54b008b52c11543aae24f",
      "511eee95982b4bb189ad0e6b54006f17",
      "8e213e0aef614f68b002b6055f4f9d93",
      "819f0ab170cd4849b8a10143241d0c14",
      "b3122089ba664927b8a6dbb0aea01248",
      "8c8ff8d14ce54f3a942fa95d78b8a70c",
      "56a85c73f5e74fbaa02a7b337de4951c",
      "e70d23e97d094b7e9da16b086f3ff597",
      "5de73f03e3f24a29a1c78951f2de55c4",
      "b6cf98059dc1496f882cd095ac4888d8",
      "c59106629a0242a08d1edb1c55e752df",
      "968c9604c05446beaadd990f8e7547f5",
      "ea3045ce7091415bb1358b12ae124b50",
      "6da0885692a748b9b437ff241f925c75",
      "371df25e456f47f2849b782339f94326",
      "9ada82198a004312a950716b84b81485",
      "0e43ea00589a47ebb1974948c836e708",
      "68f19e33f75845059662f285add8fe8f",
      "1b8cbf70501b43158a97aed37ba4ede8",
      "01b2f25c6fc44a1f866f3b1db7102a7b",
      "5961ae7462624393a390edc49b793605",
      "34e51777cf3348618340298d7b301a82",
      "e5f752e4082247098d63e72a474ed5c1",
      "22354dde38b841209ad1985028f0307b",
      "3af14bf06a1b40c6937220f2b708519b",
      "a4d5bfa114984dc0bb3500e818dbfe80",
      "273e35b7e1414782b6b11465f1d134c2",
      "b901a7f2b2df44189c8e133956f92992",
      "ec18c336b1d5420bb8a3796937effa90",
      "050543000ae647e5a421d03fbd24f6e9",
      "04063a7a2e9a41858e09e8bf7cf5576f",
      "f236360519be4f859136488c0118ae93",
      "f0f613fe94984b6d8fa3cbb675dc416a",
      "507bd3fc06cb4a5d8e7fd8bea5689060",
      "038908a677014dae80ebea27b34f5c9e",
      "d2142db554e6402ba731602077b05bd5",
      "2563a8cabb9e49a6800868c0f0060d45",
      "d0f318fbb25d40d39bbff07baf004c28",
      "e38731ae87a74cebae51823a90ec52d7",
      "009227618f0e4a9aa3390487427ff0ca",
      "881efee0895a49c5b0f2a4ab7ee30ea2",
      "1db56d05b5bf4a7ea976224972eec4b2",
      "2a681a7c8ece45cca39650254f485045",
      "3c369fe5484845739637335e9672f738",
      "dc71bace516940ba9a9979a607a6b8d1",
      "63b2e4b5768f4c2cb1aeddf867887574",
      "68c125965618436e862febccb788b2a0",
      "18a0314bfa7c40fcafab68873c76660f",
      "7b8cc928b40341e4ac5248c8e444908f",
      "6a4b4cff9ea04294bbd078d4ca72e07f",
      "6f00d75571d947d4a512934ca8e095ce",
      "ee428d43b7794cf2a32daba590991f0c",
      "cb5d3d929539476fad51773b6fc15d0e",
      "a8f95a11d36347a18fe72eafaea60899",
      "d2864d30110044eba6f47daf73f35892",
      "1892a06ff18c43028e070e0272a167ed",
      "19be4b6f83ad429998f494fb79751fe4",
      "d720b145aefe4bb4b7f228c78c98a91d"
     ]
    },
    "id": "2o3jCrdIrahb",
    "outputId": "661abdc5-76e7-456a-c329-9d789d3cf880"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9200425b0a8f4fb68ba570b144643684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/730 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61059f739d1e4653b6ce2e6a951b19b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b553c89e9e4848da9b06961dbc013412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c611c1105048bbb60c35d38e7ae5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/375 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b98ee775d73480389064eb81f2001ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5729501d73134e2188e1d28672dfa67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e849126b474fae828febb009cd9136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd9bf3e37674151928589891b452d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a68555b8fb40d0b8dfcf2085d8daa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccc09fe3f5e43209282a33e8cf12680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5489e6da731d4a418c816052effab430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819f0ab170cd4849b8a10143241d0c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371df25e456f47f2849b782339f94326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d5bfa114984dc0bb3500e818dbfe80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2563a8cabb9e49a6800868c0f0060d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a0314bfa7c40fcafab68873c76660f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, io, re, json, csv, string\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import fitz  # PyMuPDF\n",
    "from pptx import Presentation\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Models (multilingual; CPU-safe summarizer)\n",
    "summarizer = pipeline(\"summarization\", model=\"csebuetnlp/mT5_multilingual_XLSum\", device=-1)\n",
    "embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "mfta2n3xranB",
    "outputId": "4549b24e-5edb-4731-99dc-0f25f7451a12"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-a7da2b2e-cd0f-47d4-999f-7b8c1336a6a7\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-a7da2b2e-cd0f-47d4-999f-7b8c1336a6a7\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Md Touhidur Rahman_Report.docx.pdf to Md Touhidur Rahman_Report.docx.pdf\n",
      "Uploaded: Md Touhidur Rahman_Report.docx.pdf\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "up = files.upload()   # pick your PDF or PPTX\n",
    "file_path = list(up.keys())[0]\n",
    "print(\"Uploaded:\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FNipjDjAsLJJ",
    "outputId": "43912276-f20e-4406-9ea9-cf74a7be0c23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR used on 0/7 PDF pages.\n",
      "Slides: 7\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "OCR_LANG = \"eng+deu\"; OCR_PSM = \"6\"; OCR_OEM = \"3\"\n",
    "OCR_CONFIG = f\"--oem {OCR_OEM} --psm {OCR_PSM}\"\n",
    "\n",
    "def _clean_text(s):\n",
    "    s = re.sub(r\"[ \\t]+\",\" \", s or \"\"); s = re.sub(r\"\\n{3,}\",\"\\n\\n\", s); return s.strip()\n",
    "def _ocr_pil(img): return _clean_text(pytesseract.image_to_string(img, lang=OCR_LANG, config=OCR_CONFIG))\n",
    "def _pdf_page_to_image(page, dpi=300):\n",
    "    pix = page.get_pixmap(dpi=dpi)\n",
    "    return Image.open(io.BytesIO(pix.tobytes(\"png\"))).convert(\"RGB\")\n",
    "\n",
    "def extract_pdf_text(path, ocr=True, min_chars_no_ocr=25, dpi=300):\n",
    "    doc = fitz.open(path); out=[]; ocrn=0\n",
    "    for i, p in enumerate(doc):\n",
    "        t = (p.get_text() or \"\").strip(); used=False; img_only=False\n",
    "        if (not t or len(t)<min_chars_no_ocr) and ocr:\n",
    "            img = _pdf_page_to_image(p, dpi=dpi); t=_ocr_pil(img); used=True; img_only=True; ocrn+=1\n",
    "        out.append({\"index\":i,\"text\":_clean_text(t),\"source\":\"pdf\",\"ocr_used\":used,\"image_only\":img_only})\n",
    "    print(f\"OCR used on {ocrn}/{len(out)} PDF pages.\"); return out\n",
    "\n",
    "def extract_pptx_text(path, ocr=True):\n",
    "    prs = Presentation(path); out=[]; ocrn=0\n",
    "    for i, s in enumerate(prs.slides):\n",
    "        texts=[]; images=[]\n",
    "        for sh in s.shapes:\n",
    "            if hasattr(sh,\"text\") and sh.text: texts.append(sh.text)\n",
    "            if getattr(sh,\"shape_type\", None)==13 and ocr:  # PICTURE\n",
    "                try: images.append(_ocr_pil(Image.open(io.BytesIO(sh.image.blob)).convert(\"RGB\")))\n",
    "                except: pass\n",
    "        raw=_clean_text(\"\\n\".join(texts)); used=False; img_only=False\n",
    "        if ocr and (not raw or len(raw)<25) and images:\n",
    "            ocr_txt=_clean_text(\"\\n\".join([t for t in images if t]));\n",
    "            if ocr_txt: raw=(raw+\"\\n\"+ocr_txt).strip() if raw else ocr_txt; used=True; img_only=(len(_clean_text(\"\\n\".join(texts)))<5); ocrn+=1\n",
    "        out.append({\"index\":i,\"text\":raw,\"source\":\"pptx\",\"ocr_used\":used,\"image_only\":img_only})\n",
    "    print(f\"OCR used on {ocrn}/{len(out)} PPTX slides.\"); return out\n",
    "\n",
    "slides = extract_pdf_text(file_path, ocr=True) if file_path.lower().endswith(\".pdf\") else extract_pptx_text(file_path, ocr=True)\n",
    "print(\"Slides:\", len(slides))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zMsXNR75sLO7",
    "outputId": "f7863f3a-2271-47a6-fa1b-096c049aaf8d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [04:56<00:00, 42.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'index': 0,\n",
       " 'text': 'Spatio-Temporal Techniques for Image Enhancement \\nin Medical Imaging \\n \\nMd Touhidur Rahman \\nDepartment of Data Science \\nFriedrich-Alexander University Erlangen-Nuremberg, \\nBayern, Germany \\ntouhidur.rahman@fau.de \\n \\n \\nAbstract—In the field of medical diagnostics, sharp and \\naccurate images are crucial for making the right decisions. \\nMagnetic Resonance Imaging (MRI) is one of the most widely \\nused imaging methods in the world but sometimes it struggles \\nwith a critical tradeoff: achieving high spatial resolution while \\nthe scan times are short and with the least amount of artefacts \\ncaused by motion. Spatio-temporal enhancement techniques try \\nto deal with this property by using the information about \\nthe\\u2002image itself (spatial) and of its behaviour along time \\n(temporal). This report provides an in-depth review of three \\nrecently published research papers of relevance that apply\\u2002deep \\nlearning in medical images enhancement using spatio-temporal \\ninformation. \\nThe \\nselected \\nworks \\naddressed \\ndifferents \\nproblems—improving the acquisition speed of dynamic MRI, \\naiding real-time imaging during therapy,\\u2002and removing artifacts \\nin cardiac imaging. Each methods employ neural networks that \\nare trained to learn not only image frames but also\\u2002time-based \\npatterns to improve the overall image quality. By comparing \\ntheir methods, results, and applications, this report aims to \\nprovide a clear understanding of how spatio-temporal models are \\nadvancing the field of medical imaging. It also discusses current \\nlimitations and future possibilities for clinical integration. This \\nreport aims to give a clear picture of how spatio-temporal models \\nare advancing the field of medical imaging after studying and \\ncomparing their methods,results and applications. It also \\ndiscusses the current limitations and their future opportunities \\nfor the clinical implementation. \\nKeywords—Medical imaging, deep learning, spatio-temporal \\ndata, MRI enhancement, U-Net, super-resolution, motion artefact \\nreduction, dynamic MRI, real-time image processing \\n \\nI.\\u200b\\n INTRODUCTION \\nMedical imaging has become a vital\\u2002tool nowadays \\ndoctors use to diagnose and treat patients. When it comes to \\nfinding a critical factor such as a tumor and planning a surgery \\nor monitoring a patient’s\\u2002progress, clear and high quality \\nimages can make all the difference. Among many types of \\nimaging, MRI (Magnetic Resonance Imaging)\\u2002is particularly \\nvaluable as it provides high-contrast images without using \\nharmful radiation. But it does not\\u2002come without trade-offs. \\nThe higher picture quality does result in\\u2002longer scans. And \\nwhen subjects are short on\\u2002time or the patient moves — as in \\nheart or lung scans — the images can turn out blurry or partial. \\n \\nThis \\nis \\nwhere \\nspatio-temporal image enhancement \\nfeatures\\u2002comes in. The idea itself is so simple and yet so \\npowerful: rather than just\\u2002taking in one image, we take in a \\nsequence of them over time. By understanding the structure of \\nthe body(space) as well as how it moves or changes over time \\nwe can train the model to fill in missing values, or process the \\nvalues like remove noise or sharp the image quality even when \\nthe scan itself was fast or imperfect[Figure 1][4]. \\n \\nFigure\\u202f1: A noisy MR image is split into frequency components using \\nDCT, denoised via deep learning, and recombined to produce a \\ncleaner output using spatio-temporal information. \\n In this report, we have tried to find out how deep learning \\nis being applied for spatio-temporal enhancement in real world \\nmedical scenarios. We have selected three recent\\u2002research \\npapers, each tackling a different problem: one seeks to \\nenhance dynamic MRI scans, another improves real-time \\nimages during radiotherapy and a third concentrates on \\ncleaning up motion artefacts in heart scans. Despite the \\ndifferences in their goals, all three companies have in \\nmind\\u2002the same endgame: leveraging patterns across both \\nspace and time to make medical imaging faster, clearer and \\nmore dependable. Faster and clearer imaging is more essential \\nnowadays as healthcare systems deal with an increasing \\nnumber of patients and fewer radiologists. Although MRI has \\nlong been a potent diagnostic tool, doctors are still faced with \\nits drawbacks, particularly the need to balance scan speed and \\nimage quality. We are now seeing tools that can learn from a \\nsingle image as well as from the way the body moves and',\n",
       " 'source': 'pdf',\n",
       " 'ocr_used': False,\n",
       " 'image_only': False,\n",
       " 'summary': 'In our series of letters from African journalists, Dr Touhidur Rahman looks at how deep learning is being applied to improve medical imaging in real world medical scenarios. Here, he explains how spatio-temporal enhancement techniques are advancing the field.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize_text(txt):\n",
    "    txt = (txt or \"\").strip()\n",
    "    if not txt or len(txt.split()) < 25: return txt\n",
    "    return summarizer(txt, max_length=160, min_length=60, do_sample=False)[0][\"summary_text\"].strip()\n",
    "\n",
    "slide_summaries = []\n",
    "for s in tqdm(slides):\n",
    "    slide_summaries.append({**s, \"summary\": summarize_text(s[\"text\"])})\n",
    "\n",
    "with open(\"outputs/slide_summaries.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(slide_summaries, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "slide_summaries[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PGuuCkGsLSo",
    "outputId": "92d72a3f-1fd6-48d1-f8da-c642ee681f47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magnetic Resonance Imaging (MRI) is one of the most widely used imaging methods in the world but sometimes it struggles with a critical tradeoff: achieving high spatial resolution while the scan times are short and with the least amount of artefacts caused by motion. Keywords—Medical imaging, deep learning, spatio-temporal data, MRI enhancement, U-Net, super-resolution, motion artefact reduction, dynamic MRI, real-time image processing I.​ INTRODUCTION Medical imaging has become a vital tool now\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pages = [s.get(\"text\",\"\") for s in slides if s.get(\"text\")]\n",
    "doc = \"\\n\".join(pages)\n",
    "sentences = [t.strip() for t in re.split(r'(?<=[.!?])\\s+', doc) if len(t.strip().split())>=6]\n",
    "\n",
    "# filters to avoid captions/urls/watermarks\n",
    "def keep_sentence(s):\n",
    "    if sum(c.isalpha() for c in s)/max(1,len(s)) < 0.5: return False\n",
    "    if re.search(r'(Chair of|Page \\d+|\\*{3}Not for sharing\\*{3}|https?://|Wikimedia|CC BY|Pixabay)', s, re.I): return False\n",
    "    return True\n",
    "sentences = [s for s in sentences if keep_sentence(s)]\n",
    "\n",
    "stop = \"\"\"der die das ein eine und oder für von mit ohne zu auf an im in aus ist sind war waren sein auch sowie\n",
    "the a an and or for from with without into in on of to is are was were be been being this that these those it its by as at if then else when while not\n",
    "data using use based results method methods model models paper figure table slide page\"\"\".split()\n",
    "vec = TfidfVectorizer(stop_words=stop, max_df=0.9, min_df=2, ngram_range=(1,2))\n",
    "X = vec.fit_transform(sentences); scores = X.sum(axis=1).A.ravel()\n",
    "\n",
    "def mmr_select(sentences, scores, X, max_words=260, lambda_div=0.7):\n",
    "    chosen_idx=[]; remaining=set(range(len(sentences))); sim=cosine_similarity(X, X)\n",
    "    i0=int(scores.argmax()); chosen_idx.append(i0); remaining.remove(i0); total=len(sentences[i0].split())\n",
    "    while remaining and total < max_words:\n",
    "        best_i, best_v = None, -1e9\n",
    "        for i in remaining:\n",
    "            rel=scores[i]; div=0 if not chosen_idx else max(sim[i, chosen_idx])\n",
    "            val=lambda_div*rel - (1-lambda_div)*div\n",
    "            if val>best_v and total+len(sentences[i].split())<=max_words+40:\n",
    "                best_v, best_i = val, i\n",
    "        if best_i is None: break\n",
    "        chosen_idx.append(best_i); remaining.remove(best_i); total+=len(sentences[best_i].split())\n",
    "    return \" \".join(sentences[i] for i in sorted(chosen_idx))\n",
    "\n",
    "deck_summary = mmr_select(sentences, scores, X, max_words=260, lambda_div=0.7)\n",
    "deck_summary = re.sub(r'\\s+',' ', deck_summary).strip()\n",
    "with open(\"outputs/deck_summary.txt\",\"w\",encoding=\"utf-8\") as f: f.write(deck_summary)\n",
    "print(deck_summary[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCQRrFPIsRfs",
    "outputId": "4747b367-04fc-4a4c-d4f0-5267860d5559"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'slide_index': 0,\n",
       "  'score': 0.24245168268680573,\n",
       "  'preview': 'In our series of letters from African journalists, Dr Touhidur Rahman looks at how deep learning is being applied to improve medical imaging in real world medical scenarios. Here, he explains how spatio-temporal enhancement techniques are a'},\n",
       " {'slide_index': 4,\n",
       "  'score': 0.2267385721206665,\n",
       "  'preview': 'In our series of letters from across the world, Dr John Wright looks at how spatio-temporal learning (DDoS-UNet) could improve the MRI images of patients. Here, he explains how it works and what it does.'},\n",
       " {'slide_index': 2,\n",
       "  'score': 0.19386336207389832,\n",
       "  'preview': 'In our series of letters from across the world, scientists have developed a deep learning model to improve the spatio-temporal resolution of brain MRI scans. This is the first paper to be published in the Journal of Ultrasound Medicine (DDo'},\n",
       " {'slide_index': 3,\n",
       "  'score': 0.16172504425048828,\n",
       "  'preview': \"Scientists have developed a deep-learning model for brain MRIs, which allows doctors to accurately read the scans. The BBC's Tom de Castella looks at how it works, and why it doesn't need training.\"},\n",
       " {'slide_index': 5,\n",
       "  'score': 0.0890665352344513,\n",
       "  'preview': 'Scientists have published a series of research papers which reveal how spatio-temporal deep learning techniques are being used in medical imaging. Here are some of the challenges and limitations they need to work through before these techni'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [f\"Slide {s['index']}: {s['summary'] or s['text']}\" for s in slide_summaries]\n",
    "embs = embedder.encode(corpus, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "def search(query, top_k=5):\n",
    "    q = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    sims = (embs @ q.T).ravel()\n",
    "    idx = sims.argsort()[::-1][:top_k]\n",
    "    return [{\"slide_index\": slide_summaries[i][\"index\"],\n",
    "             \"score\": float(sims[i]),\n",
    "             \"preview\": (slide_summaries[i][\"summary\"] or slide_summaries[i][\"text\"])[:240]} for i in idx]\n",
    "\n",
    "search(\"Markow-Eigenschaft\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xJoCuhksRi_",
    "outputId": "f0887d55-233c-4a95-f986-b6e6e6749493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mindmap.mmd written (paste into https://mermaid.live)\n"
     ]
    }
   ],
   "source": [
    "def top_keywords(text, k=3):\n",
    "    words = re.findall(r\"[A-Za-zÄÖÜäöüß][A-Za-zÄÖÜäöüß\\-]{2,}\", text or \"\")\n",
    "    stop = set(\"\"\"der die das ein eine und oder für von mit ohne zu auf an im in aus ist sind war waren sein auch sowie\n",
    "the a an and or for from with without into in on of to is are was were be been being this that these those it its by as at if then else when while not\"\"\".split())\n",
    "    words = [w.lower() for w in words if w.lower() not in stop]\n",
    "    return [w for w,_ in Counter(words).most_common(k)]\n",
    "\n",
    "kw_to_slides = defaultdict(list)\n",
    "for s in slide_summaries:\n",
    "    base = s[\"summary\"] or s[\"text\"]\n",
    "    for kw in top_keywords(base, 3): kw_to_slides[kw].append(s[\"index\"])\n",
    "\n",
    "top_kws = list(kw_to_slides.keys())[:12]\n",
    "title = os.path.basename(file_path)\n",
    "lines = [\"mindmap\", f\"  root(({title}))\"]\n",
    "for kw in top_kws:\n",
    "    lines.append(f\"    {kw}({kw})\")\n",
    "    for idx in sorted(kw_to_slides[kw])[:6]:\n",
    "        lines.append(f\"      s{idx}([Slide {idx}])\")\n",
    "\n",
    "with open(\"outputs/mindmap.mmd\",\"w\",encoding=\"utf-8\") as f: f.write(\"\\n\".join(lines))\n",
    "print(\"mindmap.mmd written (paste into https://mermaid.live)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wuY3Gz2PsRmh",
    "outputId": "8fd5beb3-0d81-40ee-8dd9-266b37eb1439"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flashcards.csv written — 14 cards\n"
     ]
    }
   ],
   "source": [
    "def split_points(text):\n",
    "    bullets = re.findall(r\"(?:^|\\n)[\\-\\•\\*]\\s*(.+)\", text or \"\")\n",
    "    if bullets: return [b.strip() for b in bullets if len(b.strip())>3][:4]\n",
    "    sents = re.split(r\"(?<=[.!?])\\s+\", text or \"\")\n",
    "    return [s for s in sents if len(s.split())>=6][:3]\n",
    "\n",
    "def detect_de(text):\n",
    "    t=(text or \"\").lower()\n",
    "    if any(c in t for c in \"äöüÄÖÜß\"): return True\n",
    "    de_hits=sum(w in t for w in [\"und\",\"der\",\"die\",\"das\",\"ist\",\"nicht\",\"ein\",\"eine\",\"oder\",\"auch\"])\n",
    "    en_hits=sum(w in t for w in [\"and\",\"the\",\"is\",\"are\",\"not\",\"or\",\"also\",\"a\",\"an\"])\n",
    "    return de_hits>=en_hits\n",
    "\n",
    "rows=[]\n",
    "for s in slide_summaries:\n",
    "    base=s[\"summary\"] or s[\"text\"]\n",
    "    if not base: continue\n",
    "    de=detect_de(base); items=split_points(base)\n",
    "    if items:\n",
    "        for j,it in enumerate(items,1):\n",
    "            q = f\"Nenne einen Kerngedanken (Folie {s['index']}, Punkt {j}):\" if de else f\"Name one key idea (slide {s['index']}, point {j}):\"\n",
    "            rows.append([q, it, f\"ClassMind;{'DE' if de else 'EN'}\"])\n",
    "    else:\n",
    "        q = f\"Was ist die Kernaussage von Folie {s['index']}?\" if de else f\"What is the main idea of slide {s['index']}?\"\n",
    "        rows.append([q, base, f\"ClassMind;{'DE' if de else 'EN'}\"])\n",
    "\n",
    "with open(\"outputs/flashcards.csv\",\"w\",encoding=\"utf-8\",newline=\"\") as f:\n",
    "    csv.writer(f).writerow([\"Question\",\"Answer\",\"Tags\"])\n",
    "with open(\"outputs/flashcards.csv\",\"a\",encoding=\"utf-8\",newline=\"\") as f:\n",
    "    w=csv.writer(f); w.writerows(rows)\n",
    "\n",
    "print(f\"flashcards.csv written — {len(rows)} cards\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "48TW6EcssuOW",
    "outputId": "006fe858-e9f2-4171-ca45-73fd8de6a361"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_286a6694-e9c4-4c90-8dea-514858ec8e98\", \"deck_summary.txt\", 1827)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_38fa9999-33e7-444a-aab3-b17af04f8f40\", \"slide_summaries.json\", 32316)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_2f193781-c0b2-4d7c-bb08-fce06d4041a2\", \"mindmap.mmd\", 674)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_c68b003b-9406-4605-8e23-f569e22c0e05\", \"flashcards.csv\", 2442)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "for p in [\"outputs/deck_summary.txt\",\"outputs/slide_summaries.json\",\"outputs/mindmap.mmd\",\"outputs/flashcards.csv\"]:\n",
    "    try: files.download(p)\n",
    "    except Exception as e: print(\"Skip:\", p, \"->\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "id": "VkyTLENRsuTf",
    "outputId": "3edc60cd-576d-4d34-8489-04d77c9e7e60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pick the .ipynb you want to publish to GitHub:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-3103414a-99a6-407c-bdfa-39a9d71df3a2\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-3103414a-99a6-407c-bdfa-39a9d71df3a2\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Upload the .ipynb you want to publish, clean widget metadata, and save a fixed copy.\n",
    "!pip -q install nbformat==5.10.4\n",
    "import nbformat as nbf\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Pick the .ipynb you want to publish to GitHub:\")\n",
    "up = files.upload()\n",
    "name = list(up.keys())[0]\n",
    "\n",
    "nb = nbf.reads(up[name].decode(\"utf-8\"), as_version=4)\n",
    "\n",
    "# Remove repo-level widget metadata\n",
    "nb.metadata.pop(\"widgets\", None)\n",
    "# Ensure basic metadata (helps some renderers)\n",
    "nb.metadata.setdefault(\"kernelspec\", {\"name\":\"python3\",\"display_name\":\"Python 3\"})\n",
    "nb.metadata.setdefault(\"language_info\", {\"name\":\"python\"})\n",
    "\n",
    "# Remove cell-level widget metadata and ensure outputs fields are present\n",
    "for c in nb.cells:\n",
    "    if isinstance(c.get(\"metadata\", {}), dict):\n",
    "        c.metadata.pop(\"widgets\", None)\n",
    "    if c.cell_type == \"code\":\n",
    "        c.outputs = c.get(\"outputs\", []) or []   # must exist, even if empty\n",
    "        if \"execution_count\" not in c:\n",
    "            c.execution_count = None\n",
    "\n",
    "# Write cleaned notebook\n",
    "fixed = name.rsplit(\".ipynb\", 1)[0] + \"_fixed.ipynb\"\n",
    "with open(fixed, \"w\", encoding=\"utf-8\") as f:\n",
    "    nbf.write(nb, f)\n",
    "\n",
    "files.download(fixed)\n",
    "print(\"Cleaned file ready:\", fixed)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
